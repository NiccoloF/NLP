{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The goal here is to subsample the dataset, in particular we want only the reviews of italian restaurants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To subsample we use semantic search on the whole dataset using a query for italian restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### This is the code to create the embeddings for each review using a pretrained transformer for semantic search #####\n",
    "# Expected run time ~ 4 hours\n",
    "\n",
    "\n",
    "# !pip install sentence_transformers\n",
    "# from datasets import load_dataset\n",
    "# from sentence_transformers import SentenceTransformer, util\n",
    "# import torch\n",
    "\n",
    "# dataset_train=load_dataset(\"yelp_review_full\",split='train')\n",
    "# # Usual preprocessing for the text\n",
    "# def clean_sentence(sentence):\n",
    "#     sentence = re.sub(r'\\\\n',\"\",sentence)\n",
    "\n",
    "#     #removing emoticons\n",
    "#     sentence = re.sub(r'(\\:\\w+\\:|\\<[\\/\\\\]?3|[\\(\\)\\\\\\D|\\*\\$][\\-\\^]?[\\:\\;\\=]|[\\:\\;\\=B8][\\-\\^]?[3DOPp\\@\\$\\*\\\\\\)\\(\\/\\|])(?=\\s|[\\!\\.\\?]|$)',\"\", sentence)\n",
    "\n",
    "#     #removing websites\n",
    "#     sentence = re.sub(r\"(http)?s?:?\\/\\/[A-Za-z0-9^,!.\\/'+-=_?]+\", \"\", sentence)\n",
    "\n",
    "#     #numbers\n",
    "#     sentence = re.sub(r\"(\\d+)(k)\", r\"\\g<1> thousand\", sentence)\n",
    "#     sentence = re.sub(r\"(\\d+)([a-zA-z]+)\", r\"\\g<1> \\g<2>\", sentence)\n",
    "#     #convert numbers to words\n",
    "#     sentence = re.sub(r\"1\", \" one \", sentence)\n",
    "#     sentence = re.sub(r\"2\", \" two \", sentence)\n",
    "#     sentence = re.sub(r\"3\", \" three \", sentence)\n",
    "#     sentence = re.sub(r\"4\", \" four \", sentence)\n",
    "#     sentence = re.sub(r\"5\", \" five \", sentence)\n",
    "#     sentence = re.sub(r\"6\", \" six \", sentence)\n",
    "#     sentence = re.sub(r\"7\", \" seven \", sentence)\n",
    "#     sentence = re.sub(r\"8\", \" eight \", sentence)\n",
    "#     sentence = re.sub(r\"9\", \" nine \", sentence)\n",
    "#     sentence = re.sub(r\"0\", \" zero \", sentence)\n",
    "\n",
    "#     # removing extraneous symbols\n",
    "#     sentence = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=%]\", \" \", sentence)\n",
    "\n",
    "#     # expanding contraction\n",
    "#     sentence = re.sub(r\"\\'ve\", \" have \", sentence)\n",
    "#     sentence = re.sub(r\"n't\", \" not \", sentence)\n",
    "#     sentence = re.sub(r\"i'm\", \" i am \", sentence)\n",
    "#     sentence = re.sub(r\"\\'re\", \" are \", sentence)\n",
    "#     sentence = re.sub(r\"\\'d\", \" would \", sentence)\n",
    "#     sentence = re.sub(r\"\\'ll\", \" will \", sentence)\n",
    "\n",
    "#     #spacing out symbols\n",
    "#     sentence = re.sub(r\",\", \" \", sentence)\n",
    "#     sentence = re.sub(r\"\\.\", \" . \", sentence)\n",
    "#     sentence = re.sub(r\"!\", \" ! \", sentence)\n",
    "#     sentence = re.sub(r\"\\/\", \" \", sentence)\n",
    "#     sentence = re.sub(r\"\\^\", \" ^ \", sentence)\n",
    "#     sentence = re.sub(r\"\\+\", \" + \", sentence)\n",
    "#     sentence = re.sub(r\"\\-\", \" - \", sentence)\n",
    "#     sentence = re.sub(r\"\\=\", \" = \", sentence)\n",
    "#     sentence = re.sub(r\"'\", \" \", sentence)\n",
    "#     sentence = re.sub(r\":\", \" : \", sentence)\n",
    "#     sentence = re.sub(r\"%\", \" : \", sentence)\n",
    "\n",
    "#     return sentence\n",
    "\n",
    "# def preprocess_text(example):\n",
    "#     example['text'] = clean_sentence(example['text'])\n",
    "#     return example\n",
    "\n",
    "# dataset_train = dataset_train.map(preprocess_text,num_proc = 4)\n",
    "\n",
    "# # Load the pretrained model\n",
    "# model = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "# # Create embeddings\n",
    "# reviews_embeddings = model.encode(dataset_train['text'], convert_to_tensor=True, device='cuda')\n",
    "\n",
    "# # Save the embeddings tensor\n",
    "# file_path = \"embeddings_tensor.pth\"\n",
    "# torch.save(reviews_embeddings, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform semantic search\n",
    "\n",
    "from sentence_transformers import CrossEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "dataset_train=load_dataset(\"yelp_review_full\",split='train')\n",
    "labels = np.array(dataset_train['label'])\n",
    "# Calculate the indexes referring to each star \n",
    "label_map = {lab: np.where(labels == lab)[0] for lab in range(5)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also use a CrossEncoder to re-rank the search results\n",
    "\n",
    "model = SentenceTransformer('all-mpnet-base-v2')\n",
    "cross_encoder = CrossEncoder('cross-encoder/stsb-distilroberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tensor containing the embeddings of the reviews\n",
    "file_path = 'embeddings_tensor.pth'\n",
    "reviews_embeddings = torch.load(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the embedding for the query\n",
    "\n",
    "query = 'italian restaurant italia crostino carbonara cucina'\n",
    "query_embedding = model.encode(query, convert_to_tensor=True)\n",
    "\n",
    "# Normalize the embeddings to perform only the dot product to calculate the cosine similarity\n",
    "normalised_reviews_embeddings = util.normalize_embeddings(reviews_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits = util.semantic_search(query_embedding,normalised_reviews_embeddings, score_function=util.dot_score, top_k = 5000)[0]\n",
    "\n",
    "# For each review index associate its position\n",
    "doc_mapping = {num:pair['corpus_id'] for (num,pair) in enumerate(hits)}\n",
    "\n",
    "# Rerank the search results\n",
    "\n",
    "# Concatenate each found review with the query and encode them using the cross-encoder\n",
    "model_inputs = [(query, dataset_train[idx]['text']) for idx in doc_mapping.values()] \n",
    "cross_scores = cross_encoder.predict(model_inputs)\n",
    "\n",
    "# For each review index associate its new position after reranking\n",
    "new_doc_mapping = {}\n",
    "for (num,idx) in enumerate(np.argsort(-cross_scores)):\n",
    "    new_doc_mapping[num] = doc_mapping[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate for each number of stars the most relevant reviews given the query\n",
    "\n",
    "italian_restaurants_idx = []\n",
    "for i in doc_mapping.values():\n",
    "    italian_restaurants_idx.append(i)\n",
    "italian_restaurants_idx = np.array(italian_restaurants_idx) # In this way the dataset is balanced\n",
    "top300_italian_by_label = np.array([np.intersect1d(italian_restaurants_idx,label_map[lab])[:300] for lab in range(5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print 2 reviews from the top 300 reviews for each number of stars\n",
    "\n",
    "for star in range(5):\n",
    "    print('-------------------------------------------------------------------------------------')\n",
    "    print('Stars: ' + str(star+1) + '/5' )\n",
    "    num_reviews_print = 2\n",
    "    idxs = np.random.choice(300,num_reviews_print)\n",
    "    for idx in idxs:\n",
    "        print('------------ Position:' + str(idx) + '------------')\n",
    "        print(dataset_train[int(top300_italian_by_label[star][idx])]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of the labels of the italian restaurants \n",
    "subset = dataset_train.select(italian_restaurants_idx)\n",
    "stars,count = np.unique(subset['label'], return_counts = True)\n",
    "star_labels = ['1 Star', '2 Stars', '3 Stars', '4 Stars', '5 Stars']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(star_labels, count, color='skyblue', edgecolor='black')\n",
    "\n",
    "plt.title('Distribution of italian restaurant reviews')\n",
    "plt.xlabel('Review Rating')\n",
    "plt.ylabel('Number of Reviews')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Of course the expected value of the number of stars for italian restaurant is greater than 2.5 stars :)\n",
    "\n",
    "expected_value = np.dot(range(1,6),count)/np.sum(count)\n",
    "expected_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the indexes\n",
    "np.save('idxs.npy',italian_restaurants_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's generate new italian restaurant reviews by finetuning the newest LLM by META Llama 3 8B using q-LORA\n",
    "<img src=\"llama.jpeg\" alt=\"Example Image\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -U transformers\n",
    "!pip install -q -U peft\n",
    "!pip install -q -U accelerate\n",
    "!pip install -q -U datasets\n",
    "!pip install -q -U trl\n",
    "\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from time import time\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from trl import SFTTrainer,setup_chat_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"/kaggle/input/llama-3/transformers/8b-chat-hf/1\"\n",
    "# For hardware reasons we will use the 4bit quantized version and finetune with q-LORA which is LORA for quantized LLM's\n",
    "\n",
    "compute_dtype = torch.bfloat16\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=compute_dtype,\n",
    "        bnb_4bit_use_double_quant=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "\n",
    "time_start = time()\n",
    "\n",
    "model_config = AutoConfig.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    max_new_tokens=1024\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    config=model_config,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "time_end = time()\n",
    "print(f\"Prepare model, tokenizer: {round(time_end-time_start, 3)} sec.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = setup_chat_format(model, tokenizer)\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the configuration for parameter-efficient finetuning\n",
    "peft_config = LoraConfig(\n",
    "        lora_alpha=64,\n",
    "        lora_dropout=0.05,\n",
    "        r=4,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules= [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the indexes of the reviews talking about italian restaurants and select only those reviews\n",
    "italian_restaurant_idxs = np.load('/kaggle/input/italian-restaurant-idxs-new/idxs (1).npy')\n",
    "dataset_train=load_dataset(\"yelp_review_full\",split='train')\n",
    "dataset_train=dataset_train.select(italian_restaurant_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to generate the prompt for a review of given stars\n",
    "def prompt_generation(star):\n",
    "    prompt = 'Write a review of maximum 100 words of an italian restaurant in english. The review you need to write needs to be of ' + str(star) + ' stars out of 5.'\n",
    "    return prompt\n",
    "def generate_prompt(example):\n",
    "    example['prompt'] = prompt_generation(example['label'] + 1)\n",
    "    return example\n",
    "\n",
    "dataset_train = dataset_train.map(generate_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_testing():\n",
    "    for i in range(5):\n",
    "        prompt = prompt_generation(i+1)\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "        generate_ids = model.generate(inputs.input_ids, max_length=128)\n",
    "        print(tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_testing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = \"false\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "        output_dir=\"./results_llama3_sft/\",\n",
    "#         evaluation_strategy=\"steps\",\n",
    "#         do_eval=True,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        per_device_train_batch_size=8,\n",
    "        gradient_accumulation_steps=2,\n",
    "#         per_device_eval_batch_size=8,\n",
    "        log_level=\"debug\",\n",
    "        save_steps=1,\n",
    "        logging_steps=1,\n",
    "        learning_rate=8e-6,\n",
    "#         max_steps=20,\n",
    "        num_train_epochs=1,\n",
    "        warmup_steps=3,\n",
    "        lr_scheduler_type=\"linear\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SFTTrainer stands for supervised fine-tuning. The trl (Transformer Reinforcement Learning) library from HuggingFace provides a simple API to fine-tune models using SFTTrainer.\n",
    "trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=dataset_train,\n",
    "#         eval_dataset=dataset_train,\n",
    "        peft_config=peft_config,\n",
    "        dataset_text_field=\"text\",\n",
    "        max_seq_length=512,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_arguments,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
